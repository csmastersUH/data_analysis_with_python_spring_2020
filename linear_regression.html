<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]--><!--[if gt IE 8]><!--><html class="no-js" lang="en"> <!--<![endif]-->
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Machine learning: linear regression — Data analysis with Python - Spring 2020  documentation</title>
<script src="_static/js/modernizr.min.js" type="text/javascript"></script>
<script data-url_root="./" id="documentation_options" src="_static/documentation_options.js" type="text/javascript"></script>
<script src="_static/jquery.js" type="text/javascript"></script>
<script src="_static/underscore.js" type="text/javascript"></script>
<script src="_static/doctools.js" type="text/javascript"></script>
<script src="_static/language_data.js" type="text/javascript"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
<script src="_static/js/theme.js" type="text/javascript"></script>
<link href="_static/css/theme.css" rel="stylesheet" type="text/css"/>
<link href="_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="genindex.html" rel="index" title="Index"/>
<link href="search.html" rel="search" title="Search"/>
<link href="bayes.html" rel="next" title="ML: Naive Bayes classification"/>
<link href="pandas3.html" rel="prev" title="Pandas (continues)"/>
</head>
<body class="wy-body-for-nav">
<div class="wy-grid-for-nav">
<nav class="wy-nav-side" data-toggle="wy-nav-shift">
<div class="wy-side-scroll">
<div class="wy-side-nav-search"><a href="http://www.helsinki.fi" style="margin-bottom: 0px;"><img src="https://uni.materialbank.net/NiboWEB/uni/getPublicFile.do?uuid=146263&amp;inline=false&amp;ticket=8a2a112700dc87abd2813d55e149bc0c&amp;type=original" style="margin-bottom: 0px;max-width: 60%;height: auto;width: auto;"/></a>
<a class="icon icon-home" href="index.html"> Data analysis with Python - Spring 2020
          

          
          </a>
<div role="search">
<form action="search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div>
<div aria-label="main navigation" class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation">
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="instructions.html">Initializing course environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="tools.html">Running Python code</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently asked questions</a></li>
</ul>
<p class="caption"><span class="caption-text">Week 1:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="basics.html">Python</a></li>
</ul>
<p class="caption"><span class="caption-text">Week 2:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="basics2.html">Python (continues)</a></li>
<li class="toctree-l1"><a class="reference internal" href="numpy.html">NumPy</a></li>
</ul>
<p class="caption"><span class="caption-text">Week 3:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="numpy2.html">NumPy (continues)</a></li>
<li class="toctree-l1"><a class="reference internal" href="matplotlib.html">Matplotlib</a></li>
<li class="toctree-l1"><a class="reference internal" href="image_processing.html">Image processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="pandas1.html">Pandas</a></li>
</ul>
<p class="caption"><span class="caption-text">Week 4:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="pandas2.html">Pandas (continues)</a></li>
</ul>
<p class="caption"><span class="caption-text">Week 5:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="pandas3.html">Pandas (continues)</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Machine learning: linear regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Linear-regression">Linear regression</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Multiple-features">Multiple features</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Polynomial-regression">Polynomial regression</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#Additional-information">Additional information</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Summary-(week-5)">Summary (week 5)</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Week6:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="bayes.html">ML: Naive Bayes classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="clustering.html">ML: Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="pca.html">ML: Principal component analysis</a></li>
</ul>
<p class="caption"><span class="caption-text">Week7:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="project.html">Project work</a></li>
<li class="toctree-l1"><a class="reference internal" href="project.html#running-tests-when-peer-reviewing-students-notebooks">Running tests when peer-reviewing students notebooks</a></li>
</ul>
</div>
</div>
</nav>
<section class="wy-nav-content-wrap" data-toggle="wy-nav-shift">
<nav aria-label="top navigation" class="wy-nav-top">
<i class="fa fa-bars" data-toggle="wy-nav-top"></i>
<a href="index.html">Data analysis with Python - Spring 2020</a>
</nav>
<div class="wy-nav-content">
<div class="rst-content">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="wy-breadcrumbs">
<li><a href="index.html">Docs</a> »</li>
<li>Machine learning: linear regression</li>
<li class="wy-breadcrumbs-aside">
<a href="_sources/linear_regression.ipynb.txt" rel="nofollow"> View page source</a>
</li>
</ul>
<hr/>
</div>
<div class="document" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<div itemprop="articleBody">
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 5ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="admonition note">
This page was generated from <a class="reference external" href="https://github.com/csmastersUH/data_analysis_with_python_spring_2020/blob/master/linear_regression.ipynb">linear_regression.ipynb</a>.
<span class="raw-html"><br/><a href="https://colab.research.google.com/github/csmastersUH/data_analysis_with_python_spring_2020/blob/master/linear_regression.ipynb"><img align="left" alt="Open in Colab" src="https://colab.research.google.com/assets/colab-badge.svg" title="Open and Execute in Google Colaboratory"/></a></span>
<span class="raw-html"><br/></span></div>
<!--NAVIGATION--><table border="1" class="docutils">
<colgroup>
<col width="33%"/>
<col width="33%"/>
<col width="33%"/>
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head"><ul class="first last simple">
<li></li>
</ul>
</th>
<th class="head"><ul class="first last simple">
<li></li>
</ul>
</th>
<th class="head"><ul class="first last simple">
<li></li>
</ul>
</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><a class="reference external" href="#Exercise-10-(linear-regression)">Exercise 10 (linear regression)</a></td>
<td><a class="reference external" href="#Exercise-11-(mystery-data)">Exercise 11 (mystery data)</a></td>
<td><a class="reference external" href="#Exercise-12-(coefficient-of-determination)">Exercise 12 (coefficient of determination)</a></td>
</tr>
<tr class="row-odd"><td><a class="reference external" href="#Exercise-13-(cycling-weather-continues)">Exercise 13 (cycling weather continues)</a></td>
<td> </td>
<td> </td>
</tr>
</tbody>
</table>
<div class="section" id="Machine-learning:-linear-regression">
<h1>Machine learning: linear regression<a class="headerlink" href="#Machine-learning:-linear-regression" title="Permalink to this headline">¶</a></h1>
<div class="section" id="Linear-regression">
<h2>Linear regression<a class="headerlink" href="#Linear-regression" title="Permalink to this headline">¶</a></h2>
<p>Regression analysis tries to explain relationships between variables. One of these variables, called dependend variable, is what we want to “explain” using one or more <em>explanatory variables</em>. In linear regression we assume that the dependent variable can be, approximately, expressed as a linear combination of the explanatory variables. As a simple example, we might have dependent variable height and an explanatory variable age. The age of a person can quite well explain the height of a person,
and this relationship is approximately linear for kids (ages between 1 and 16). Another way of thinking about regression is fitting a curve to the observed data points. If we have only one explanatory variable, then this is easy to visualize, as we shall see below.</p>
<p>We can apply the linear regression easily with the <a class="reference external" href="https://scikit-learn.org/stable/">scikit-learn</a> package. Let’s go through some examples.</p>
<p>First we make the usual standard imports.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">sklearn</span>   <span class="c1"># This imports the scikit-learn library</span>
</pre></div>
</div>
</div>
<p>Then we create some data with approximately the relationship <span class="math notranslate nohighlight">\(y=2x+1\)</span>, with normally distributed errors.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">n</span><span class="o">=</span><span class="mi">20</span>   <span class="c1"># Number of data points</span>
<span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">y</span><span class="o">=</span><span class="n">x</span><span class="o">*</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c1"># Standard deviation 1</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[  0.           0.52631579   1.05263158   1.57894737   2.10526316
   2.63157895   3.15789474   3.68421053   4.21052632   4.73684211
   5.26315789   5.78947368   6.31578947   6.84210526   7.36842105
   7.89473684   8.42105263   8.94736842   9.47368421  10.        ]
[  2.76405235   2.45278879   4.08400114   6.39878794   7.07808431
   5.28588001   8.26587789   8.21706384   9.31783378  10.88428271
  11.67035936  14.03322088  14.39261667  14.80588554  16.18070534
  17.12314801  19.33618434  18.68957858  20.26043612  20.14590426]
</pre></div></div>
</div>
<p>Next we import the <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> class.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
</pre></div>
</div>
</div>
<p>Now we can fit a line through the data points (x, y):</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">model</span><span class="o">=</span><span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">xfit</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">yfit</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xfit</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xfit</span><span class="p">,</span><span class="n">yfit</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"black"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="s1">'o'</span><span class="p">)</span>
<span class="c1"># The following will draw as many line segments as there are columns in matrices x and y</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])]),</span> <span class="n">color</span><span class="o">=</span><span class="s2">"red"</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/linear_regression_9_0.png" src="_images/linear_regression_9_0.png"/>
</div>
</div>
<p>The linear regression tries to minimize the sum of squared errors <span class="math notranslate nohighlight">\(\sum_i (y[i] - \hat{y}[i])^2\)</span>; this is the sum of the squared lengths of the red line segments in the above plot. The estimated values <span class="math notranslate nohighlight">\(\hat{y}[i]\)</span> are denoted by <code class="docutils literal notranslate"><span class="pre">yfit[i]</span></code> in the above code.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"Parameters:"</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Coefficient:"</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Intercept:"</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Parameters: [ 1.88627741] 2.13794752053
Coefficient: 1.88627741448
Intercept: 2.13794752053
</pre></div></div>
</div>
<p>In this case, the coefficient is the slope of the fitted line, and the intercept is the point where the fitted line intersects with the y-axis.</p>
<div class="admonition warning">
Note that in scikit-learn the attributes of the model that store the learned parameters have always an underscore at the end of the name. This applies to all algorithms in sklearn, not only the linear regression. This naming style allows one to easily spot the learned model parameters from other attributes.</div>
<p>The parameters estimated by the regression algorithm were quite close to the parameters that generated the data: coefficient 2 and intercept 1. Try experimenting with the number of data points and/or the standard deviation, to see if you can improve the estimated parameters.</p>
<div class="section" id="Multiple-features">
<h3>Multiple features<a class="headerlink" href="#Multiple-features" title="Permalink to this headline">¶</a></h3>
<p>The previous example had only one explanatory variable. Sometimes this is called a <em>simple linear regression</em>. The next example illustrates a more complex regression with multiple explanatory variables.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">sample1</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>   <span class="c1"># The three explanatory variables have values 1, 2, and 3, respectively</span>
<span class="n">sample2</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">])</span>   <span class="c1"># Another example of values of explanatory variables</span>
<span class="n">sample3</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">10</span><span class="p">])</span>   <span class="c1"># ...</span>
<span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">15</span><span class="p">,</span><span class="mi">39</span><span class="p">,</span><span class="mi">66</span><span class="p">])</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>   <span class="c1"># For values 1,2, and 3 of explanatory variables, the value y=15 was observed, and so on.</span>
</pre></div>
</div>
</div>
<p>Let’s try to fit a linear model to these points:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">model2</span><span class="o">=</span><span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">sample1</span><span class="p">,</span><span class="n">sample2</span><span class="p">,</span><span class="n">sample3</span><span class="p">])</span>
<span class="n">model2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">model2</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">model2</span><span class="o">.</span><span class="n">intercept_</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>(array([  5.69493795e+00,   3.36972233e+00,   4.20919214e-03]), 0.0)
</pre></div>
</div>
</div>
<p>Let’s print the various components involved.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">b</span><span class="o">=</span><span class="n">model2</span><span class="o">.</span><span class="n">coef_</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"x:</span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"b:</span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"y:</span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span> <span class="n">y</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"product:</span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
x:
 [[ 1  2  3]
 [ 4  5  6]
 [ 7  8 10]]
b:
 [[  5.69493795e+00]
 [  3.36972233e+00]
 [  4.20919214e-03]]
y:
 [[ 12.44701018]
 [ 39.6536186 ]
 [ 66.8644362 ]]
product:
 [[ 12.44701018]
 [ 39.6536186 ]
 [ 66.8644362 ]]
</pre></div></div>
</div>
</div>
<div class="section" id="Polynomial-regression">
<h3>Polynomial regression<a class="headerlink" href="#Polynomial-regression" title="Permalink to this headline">¶</a></h3>
<p>It may perhaps come as a surprise that one can fit a polynomial curve to data points using linear regression. The trick is to add new explanatory variables to the model. Below we have a single feature x with associated y values given by third degree polynomial, with some (gaussian) noise added. It is clear from the below plot that we cannot explain the data well with a linear function. We add two new features: <span class="math notranslate nohighlight">\(x^2\)</span> and <span class="math notranslate nohighlight">\(x^3\)</span>. Now the model has three explanatory variables,
<span class="math notranslate nohighlight">\(x, x^2\)</span> and <span class="math notranslate nohighlight">\(x^3\)</span>. The linear regression will find the coefficients for these variables.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">50</span><span class="p">,</span><span class="mi">150</span><span class="p">,</span><span class="mi">50</span><span class="p">)</span>
<span class="n">y</span><span class="o">=</span><span class="mf">0.15</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">3</span> <span class="o">-</span> <span class="mi">20</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">5</span><span class="o">*</span><span class="n">x</span> <span class="o">-</span> <span class="mi">4</span> <span class="o">+</span> <span class="mi">5000</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"black"</span><span class="p">)</span>
<span class="n">model_linear</span><span class="o">=</span><span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model_squared</span><span class="o">=</span><span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model_cubic</span><span class="o">=</span><span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">x2</span><span class="o">=</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span>
<span class="n">x3</span><span class="o">=</span><span class="n">x</span><span class="o">**</span><span class="mi">3</span>
<span class="n">model_linear</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">model_squared</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">x2</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">model_cubic</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="n">x3</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">xf</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">50</span><span class="p">,</span><span class="mi">150</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">yf_linear</span><span class="o">=</span><span class="n">model_linear</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">yf_squared</span><span class="o">=</span><span class="n">model_squared</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">x2</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">yf_cubic</span><span class="o">=</span><span class="n">model_cubic</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="n">x3</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xf</span><span class="p">,</span><span class="n">yf_linear</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"linear"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xf</span><span class="p">,</span><span class="n">yf_squared</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"squared"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xf</span><span class="p">,</span><span class="n">yf_cubic</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"cubic"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Coefficients:"</span><span class="p">,</span> <span class="n">model_cubic</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Intercept:"</span><span class="p">,</span> <span class="n">model_cubic</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Coefficients: [-36.65414588 -20.17228669   0.15359003]
Intercept: -167.160466064
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/linear_regression_22_1.png" src="_images/linear_regression_22_1.png"/>
</div>
</div>
<p>Linear and squared are not enough to explain the data, but the linear regression manages to fit quite well a polynomial curve to the data points, when cubic variables are included!</p>
<hr class="docutils"/>
<div class="admonition note"><a id="Exercise-10-(linear-regression)">Exercise 10 (linear regression)</a></div>
<p>This exercise can give two points at maximum!</p>
<p>Part 1.</p>
<p>Write a function <code class="docutils literal notranslate"><span class="pre">fit_line</span></code> that gets one dimensional arrays <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> as parameters. The function should return the tuple <code class="docutils literal notranslate"><span class="pre">(slope,</span> <span class="pre">intercept)</span></code> of the fitted line. Write a main program that tests the <code class="docutils literal notranslate"><span class="pre">fit_line</span></code> function with some example arrays. The main function should produce output in the following form:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Slope</span><span class="p">:</span> <span class="mf">1.0</span>
<span class="n">Intercept</span><span class="p">:</span> <span class="mf">1.16666666667</span>
</pre></div>
</div>
<p>Part 2.</p>
<p>Modify your <code class="docutils literal notranslate"><span class="pre">main</span></code> function to plot the fitted line using matplotlib, in addition to the textual output. Plot also the original data points.</p>
<hr/><hr class="docutils"/>
<div class="admonition note"><a id="Exercise-11-(mystery-data)">Exercise 11 (mystery data)</a></div>
<p>Read the tab separated file <code class="docutils literal notranslate"><span class="pre">mystery_data.tsv</span></code>. Its first five columns define the features, and the last column is the response. Use scikit-learn’s <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> to fit this data. Implement function <code class="docutils literal notranslate"><span class="pre">mystery_data</span></code> that reads this file and learns and returns the regression coefficients for the five features. You don’t have to fit the intercept. The <code class="docutils literal notranslate"><span class="pre">main</span></code> method should print output in the following form:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Coefficient</span> <span class="n">of</span> <span class="n">X1</span> <span class="ow">is</span> <span class="o">...</span>
<span class="n">Coefficient</span> <span class="n">of</span> <span class="n">X2</span> <span class="ow">is</span> <span class="o">...</span>
<span class="n">Coefficient</span> <span class="n">of</span> <span class="n">X3</span> <span class="ow">is</span> <span class="o">...</span>
<span class="n">Coefficient</span> <span class="n">of</span> <span class="n">X4</span> <span class="ow">is</span> <span class="o">...</span>
<span class="n">Coefficient</span> <span class="n">of</span> <span class="n">X5</span> <span class="ow">is</span> <span class="o">...</span>
</pre></div>
</div>
<p>Which features you think are needed to explain the response Y?</p>
<hr/><hr class="docutils"/>
<div class="admonition note"><a id="Exercise-12-(coefficient-of-determination)">Exercise 12 (coefficient of determination)</a></div>
<p>This exercise can give two points at maximum!</p>
<p>Using the same data as in the previous exercise, instead of printing the regression coefficients, print the <em>coefficient of determination</em>. The coefficient of determination, denoted by R2, tells how well the linear regression fits the data. The maximum value of the coefficient of determination is 1. That means the best possible fit.</p>
<p>Part 1.</p>
<p>Using all the features (X1 to X5), fit the data using a linear regression (include the intercept). Get the coefficient of determination using the <code class="docutils literal notranslate"><span class="pre">score</span></code> method of the <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> class. Write a function <code class="docutils literal notranslate"><span class="pre">coefficient_of_determination</span></code> to do all this. It should return a list containing the R2-score as the only value.</p>
<p>Part 2.</p>
<p>Extend your function so that it also returns R2-scores related to linear regression with each single feature in turn. The <code class="docutils literal notranslate"><span class="pre">coefficient_of_determination</span></code> (<a class="reference external" href="https://en.wikipedia.org/wiki/Coefficient_of_determination">https://en.wikipedia.org/wiki/Coefficient_of_determination</a>) function should therefore return a list with six R2-scores (the first score is for five features, like in Part 1). To achieve this, your function should call both the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method and the <code class="docutils literal notranslate"><span class="pre">score</span></code> method six times.</p>
<p>The output from the main method should look like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">R2</span><span class="o">-</span><span class="n">score</span> <span class="k">with</span> <span class="n">feature</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="n">X</span><span class="p">:</span> <span class="o">...</span>
<span class="n">R2</span><span class="o">-</span><span class="n">score</span> <span class="k">with</span> <span class="n">feature</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="n">X1</span><span class="p">:</span> <span class="o">...</span>
<span class="n">R2</span><span class="o">-</span><span class="n">score</span> <span class="k">with</span> <span class="n">feature</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="n">X2</span><span class="p">:</span> <span class="o">...</span>
<span class="n">R2</span><span class="o">-</span><span class="n">score</span> <span class="k">with</span> <span class="n">feature</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="n">X3</span><span class="p">:</span> <span class="o">...</span>
<span class="n">R2</span><span class="o">-</span><span class="n">score</span> <span class="k">with</span> <span class="n">feature</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="n">X4</span><span class="p">:</span> <span class="o">...</span>
<span class="n">R2</span><span class="o">-</span><span class="n">score</span> <span class="k">with</span> <span class="n">feature</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="n">X5</span><span class="p">:</span> <span class="o">...</span>
</pre></div>
</div>
<p>How small can the R2-score be? Experiment both with fitting the intercept and without fitting the intercept.</p>
<hr/><hr class="docutils"/>
<div class="admonition note"><a id="Exercise-13-(cycling-weather-continues)">Exercise 13 (cycling weather continues)</a></div>
<p>Write function <code class="docutils literal notranslate"><span class="pre">cycling_weather_continues</span></code> that tries to explain with linear regression the variable of a cycling measuring station’s counts using the weather data from corresponding day. The function should take the name of a (cycling) measuring station as a parameter and return the regression coefficients and the score. In more detail:</p>
<p>Read the weather data set from the <code class="docutils literal notranslate"><span class="pre">src</span></code> folder. Read the cycling data set from folder <code class="docutils literal notranslate"><span class="pre">src</span></code> and restrict it to year 2017. Further, get the sums of cycling counts for each day. Merge the two datasets by the year, month, and day. Note that for the above you need only small additions to the solution of exercise <code class="docutils literal notranslate"><span class="pre">cycling_weather</span></code>. After this, use forward fill to fill the missing values.</p>
<p>In the linear regression use as explanatory variables the following columns <code class="docutils literal notranslate"><span class="pre">'Precipitation</span> <span class="pre">amount</span> <span class="pre">(mm)'</span></code>, <code class="docutils literal notranslate"><span class="pre">'Snow</span> <span class="pre">depth</span> <span class="pre">(cm)'</span></code>, and <code class="docutils literal notranslate"><span class="pre">'Air</span> <span class="pre">temperature</span> <span class="pre">(degC)'</span></code>. Explain the variable (measuring station), whose name is given as a parameter to the function <code class="docutils literal notranslate"><span class="pre">cycling_weather_continues</span></code>. Fit also the intercept. The function should return a pair, whose first element is the regression coefficients and the second element is the score. Above, you may need to use the method <code class="docutils literal notranslate"><span class="pre">reset_index</span></code> (its
counterpart is the method <code class="docutils literal notranslate"><span class="pre">set_index</span></code>).</p>
<p>The output from the <code class="docutils literal notranslate"><span class="pre">main</span></code> function should be in the following form:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Measuring</span> <span class="n">station</span><span class="p">:</span> <span class="n">x</span>
<span class="n">Regression</span> <span class="n">coefficient</span> <span class="k">for</span> <span class="n">variable</span> <span class="s1">'precipitation'</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">x</span>
<span class="n">Regression</span> <span class="n">coefficient</span> <span class="k">for</span> <span class="n">variable</span> <span class="s1">'snow depth'</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">x</span>
<span class="n">Regression</span> <span class="n">coefficient</span> <span class="k">for</span> <span class="n">variable</span> <span class="s1">'temperature'</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">x</span>
<span class="n">Score</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">xx</span>
</pre></div>
</div>
<p>Use precision of one decimal for regression coefficients, and precision of two decimals for the score. In the <code class="docutils literal notranslate"><span class="pre">main</span></code> function test you solution using some measuring station, for example <code class="docutils literal notranslate"><span class="pre">Baana</span></code>.</p>
<hr/></div>
</div>
<div class="section" id="Additional-information">
<h2>Additional information<a class="headerlink" href="#Additional-information" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>The <a class="reference external" href="https://scikit-learn.org/stable/">scikit-learn</a> library concentrates on machine learning. Check out library <a class="reference external" href="http://www.statsmodels.org/stable/index.html">statsmodels</a> for a more statistical viewpoint to regression.</li>
</ul>
</div>
<div class="section" id="Summary-(week-5)">
<h2>Summary (week 5)<a class="headerlink" href="#Summary-(week-5)" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">pd.concat</span></code> and <code class="docutils literal notranslate"><span class="pre">pd.merge</span></code> can both combine two DataFrames, but the way the combining is done differs. The function <code class="docutils literal notranslate"><span class="pre">pd.concat</span></code> concatenates based on <em>indices</em> of DataFrames, whereas <code class="docutils literal notranslate"><span class="pre">pd.merge</span></code> combines based on the <em>content</em> of common variable(s).</li>
<li>The option <code class="docutils literal notranslate"><span class="pre">join="outer</span></code> to <code class="docutils literal notranslate"><span class="pre">pd.concat</span></code> can create missing values, but <code class="docutils literal notranslate"><span class="pre">join=inner</span></code> cannot. The former gives the union of indices and the latter gives the intersection of indices.</li>
<li>With <code class="docutils literal notranslate"><span class="pre">pd.concat</span></code> overlapping indices can:<ul>
<li>cause an error</li>
<li>cause renumbering of indices</li>
<li>create hierarchical indices</li>
</ul>
</li>
<li>Merging can join elements<ul>
<li>one-to-one</li>
<li>one-to-many</li>
<li>many-to-many</li>
</ul>
</li>
<li>In grouping a DataFrame can be thought to be split into smaller DataFrames. The major classes of operations on these groups are:<ul>
<li>aggregate</li>
<li>filter</li>
<li>transform (retains shape)</li>
<li>apply</li>
</ul>
</li>
<li>Series which are indexed by time are called time series</li>
<li>Linear regression can be used to find out linear relationships between variables<ul>
<li>can have more than one feature (explanatory variable)</li>
<li>fitting polynomials is still linear regression</li>
</ul>
</li>
</ul>
<!--NAVIGATION--></div>
</div>
</div>
</div>
<footer>
<div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
<a accesskey="n" class="btn btn-neutral float-right" href="bayes.html" rel="next" title="ML: Naive Bayes classification">Next <span class="fa fa-arrow-circle-right"></span></a>
<a accesskey="p" class="btn btn-neutral float-left" href="pandas3.html" rel="prev" title="Pandas (continues)"><span class="fa fa-arrow-circle-left"></span> Previous</a>
</div>
<hr/>
<div role="contentinfo">
<p>
        © Copyright 2018-2020, Jarkko Toivonen

    </p>
</div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
</div>
</div>
</section>
</div>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
</body>
</html>